
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}
\label{sec:related}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


There is a great deal of work related to this thesis. We have touched some most
relevant work (notably intersection types) in \cref{chap:background}. In this
chapter, we briefly review other related work, starting with a summary of two
most common approaches on coherence (\cref{sec:related:coherence}). We then
consider various existing mechanisms to foster modularity and code reuse in the
rest of this chapter.


\section{Coherence}
\label{sec:related:coherence}

In calculi that feature coercive subtyping, a semantics that interprets the
subtyping judgment by introducing explicit coercions is typically defined on
typing derivations rather than on typing judgments. A natural question that
arises for such systems is whether the semantics is \emph{coherent}, i.e.,
distinct typing derivations of the same typing judgment possess the same
meaning. Since \citet{Reynolds_1991} proved the coherence of a calculus with
intersection types, based on the denotational semantics for intersection types,
many researchers have studied the problem of coherence in a variety of typed
calculi. Below we summarize two commonly-found approaches in the literature.

\subsection{Normalization-based Approach}
The first approach is based on normalization. \citet{Breazu_Tannen_1991} proved
the coherence of a coercion translation from
\textsf{Fun}~\citep{cardelli1985understanding} extended with recursive types to
System F by showing that any two typing derivations of the same judgment are
normalizable to a unique normal derivation where the correctness of the
normalization steps is justified by an equational theory in System F.
\citet{Curien_1992} presented a translation of System F$_\leq$ into a calculus
with explicit coercions and showed that any derivations of the same judgment are
translated to terms that are normalizable to a unique normal form. Following the
same approach, \citet{SCHWINGHAMMER_2008} proved the coherence of coercion
translation from \citeauthor{Moggi_1991}'s computational lambda calculus~\citep{Moggi_1991} with
subtyping.


\subsection{Context-based Approach}

Central to the first approach is to find a normal form for a representation of
the derivation and show that normal forms are unique for a given typing
judgment. However, this approach cannot be directly applied to Curry-style
calculi, i.e., where the lambda abstractions are not type annotated. Also this
line of reasoning cannot be used when the calculus has general recursion.
\citet{biernacki2015logical} considered the coherence problem of coercion
semantics. Their criterion for coherence of the translation is
\emph{contextual equivalence} in the target calculus. They presented a
construction of logical relations for establishing so constructed coherence for
coercion semantics, showing that this approach is applicable in a variety of
calculi, including delimited continuations and control-effect subtyping.

As far as we know, our work is the first to use logical relations to show the
coherence for intersection types and the merge operator. BCD subtyping in
our setting poses a non-trivial complication over
\citeauthor{biernacki2015logical}'s simple structural subtyping. Indeed, because
any two coercions between given types are behaviorally equivalent in the target
language, their coherence reasoning can all take place in the target language.
This is not true in our setting, where coercions can be distinguished by
arbitrary target programs, but not those that are elaborations of source
programs. (Recall that $ [[\x . pp1 x]]    $ and $ [[ \x . pp2 x]]    $ should be equated in our setting.)
Hence, we have to restrict our reasoning to the latter class, which is reflected
in a more complicated notion of contextual equivalence and our logical
relation's non-trivial treatment of pairs. They also did not study parametric
polymorphism, which requires extra effort in the proof.


\section {BCD Subtyping and Decidability}

The BCD type system was first introduced by \citet{Barendregt_1983}. It is
derived from a filter lambda model in order to characterize exactly the strongly
normalizing terms. The BCD type system features a powerful subtyping relation,
which serves as a base for our subtyping relation.
\citet{DBLP:journals/corr/BessaiDDCd15} show how to type classes and mixins in a
BCD-style record calculus with a merge-like operator~\citep{bracha1990mixin}
that only operates on records, and they only study a type assignment system. The
decidability of BCD subtyping has been shown in several
works~\citep{pierce1989decision, Kurata_1995, Rehof_2011, Statman_2015}.
\citet{laurent2012intersection} formalized the relation in Coq in order to
eliminate transitivity cuts from it, but his formalization does not deliver an
algorithm. Only recently, \citet{Laurent18b} presents a general way of defining
a BCD-like subtyping relation extended with generic contravariant/covariant type
constructors that enjoys the ``sub-formula property'' (read decidability). The
key idea is to generalize the form of subtyping from $[[A <: B]]$ to $[[A1]],
\dots, [[An]] \vdash [[B]]$, which is interpreted as meaning $[[ A1 & ... & An <: B]]$.
Here is his subtyping system instantiated with singleton records, adapted to our setting:
\begin{mathpar}
  \inferrule*{ }{ [[pri]] \vdash [[pri]]  } \and
  \inferrule*{ }{ \vdash [[Top]]  } \and
  \inferrule*{ \vdash [[B]]  }{ \vdash [[A -> B]]  } \and
  \inferrule*{ \vdash [[A]]  }{ \vdash [[ {l : A}   ]]  } \and
  \inferrule*{ \Gamma, \Delta \vdash [[C]]  }{ \Gamma, \rho, \Delta \vdash [[C]] } \and
  \inferrule*{ \Gamma, \Delta \vdash [[C]]  }{ \Gamma, [[Top]], \Delta \vdash [[C]] } \and
  \inferrule*{ \Gamma, \Delta \vdash [[C]]  }{ \Gamma, [[A -> B]], \Delta \vdash [[C]] } \and
  \inferrule*{ \Gamma, \Delta \vdash [[C]]  }{ \Gamma, [[ {l : B}  ]], \Delta \vdash [[C]] } \and
  \inferrule*{ \Gamma \vdash [[A]] \\ \Gamma \vdash [[B]]  }{ \Gamma \vdash [[A & B]] } \and
  \inferrule*{ \Gamma, [[A]], [[B]], \Delta \vdash [[C]]  }{ \Gamma, [[A & B]], \Delta \vdash [[C]] } \and
  \inferrule*{ [[A]] \vdash [[A1]] \\ \dots \\ [[A]] \vdash [[An]] \\  [[B1]], \dots, [[Bn]] \vdash [[B]]  } { [[A1 -> B1]], \dots, [[An -> Bn]] \vdash [[A -> B]]  } \and
  \inferrule*{ [[A1]], \dots , [[An]] \vdash [[B]]  } {  [[ {l : A1} ]], \dots, [[{l : An}]] \vdash [[ {l : B} ]]  }
\end{mathpar}
The first two rules are the base cases. The third and forth rules deal with
cases where $[[B]]$ is a ``top-like'' type. The next four rules are the
weakening rules for integers, top types, function types and singleton records.
The next two rules are the introduction and elimination rules for intersections.
The last two rules combine the function distributivity rule with usual function
subtyping, and record distributivity rule with usual record subtyping,
respectively. \citeauthor{Laurent18b} proved in Coq that $[[ A ]] \vdash [[B]]$
if and only if $[[ A <: B ]]$. % Based on \citeauthor{Statman_2015}'s work~\citep{Statman_2015},
% \citet{bessaiextracting} present a formally verified subtyping algorithm in Coq.
Our Coq formalization follows a different idea based on Pierce's decision
procedure~\citep{pierce1989decision}, which is shown to be easily extensible to
coercions and records. In the course of our mechanization we identified several
mistakes in Pierce's proofs, as well as some important missing lemmas. Finally,
it would be interesting to study an efficient subtyping algorithm in normal
practice. As noted by \citet{reynolds1997design}, however, the worst-case
inefficiency is inevitable. In fact, any typechecker for languages using
intersection types is PSPACE-hard.




\section{Intersection Types and the Merge Operator}

Forsythe~\citep{reynolds1988preliminary} has intersection types and a merge-like
operator. However to ensure coherence, various restrictions were added to limit
the use of merges. Forsythe only permits $p_1 ,, p_2$ when $p_2$ is either an
lambda abstraction or a record, whose meaning ``overrides'' the corresponding
type of meaning of $p_1$. For instance, there is a rule regarding lambda
abstraction that says (adapted to our syntax):
\[
  \inferrule*{ \Gamma \vdash \lam{x}{p_2} : \theta_1 \rightarrow \theta_2  }{ \Gamma \vdash (p_1 ,, \lam{x}{p_2}) : \theta_1 \rightarrow \theta_2  }
\]
which means that in a merge of two functions, the second one always takes
precedence to the first one. In contrast, our typing rule for merges is more
fine-grained in the sense that both functions are retained as long as they are
disjoint. \citet{Castagna_1992} proposed a coherent calculus $\lambda \&$ to
study overloading functions. Interestingly, $\lambda \&$ has a special merge
operator that works on functions only. Like ours, they also impose
well-formedness conditions on the formation of a (functional) merge. However,
those conditions operate on function types only, and it is not clear how to
generalize them to arbitrary types. \citet{dunfield2014elaborating} shows
significant expressiveness of type systems with (unrestricted) intersection
types and a merge operator. However, his calculus lacks coherence. The limitation
was addressed by \citet{oliveira2016disjoint}, who introduced disjointness to
ensure coherence.

Compared to prior work on disjoint intersection types, the approach in this thesis simplifies calculi with
disjoint intersection types by removing several restrictions. Furthermore, our
calculi adopt a more powerful subtyping relation based on BCD subtyping, which
in turn requires the use of a more sophisticated proof method for proving
coherence. On a pragmatic note, dynamic inheritance, self-references and
abstract methods are all missing from prior work, but, as shown in this thesis,
they can be encoded using an elaboration that employs ideas from the
denotational model of inheritance~\citep{cook1989denotational}.



\section{Intersection Types and Polymorphism}
\label{chap:poly:bcd}

\citet{pierce1991programming} proposed $\mathsf{F}_{\land}$, a calculus
combining intersection types and bounded quantification. $\mathsf{F}_{\land}$
also adopts a BCD-like subtyping relation. Moreover, since the
$\forall$-quantifier behaves somewhat like an arrow constructor,
\citeauthor{pierce1991programming} added a new rule which allows intersections
to be distributed over quantifiers on the right-hand side.
\[
\inferrule*{  }{  \inter{ (\forall (\alpha <: [[A]]) .\ [[B1]]) }{ (\forall (\alpha <: [[A]]) .\ [[B2]]) }  <: \forall (\alpha <: [[A]]) .\ \inter{[[B1]]}{[[B2]]}  }
\]
In \fnamee, we do not have this distributivity rule, but are thinking to add
a similar rule, as shown below:
\[
\inferrule*[lab=distPoly]{  }{  [[  (\X ** A . B1) &  (\X ** A . B2)  <: \X ** A . B1 & B2 ]]       }
\]
This rule will add extra expressiveness to \fnamee, e.g., we can compose
algebras with polymorphic components:
\[
[[ {l : \ X . X -> A} & {l : \X . X -> B} <: {l : \ X . X -> A & B} ]]
\]
We expect this rule would not pose any difficulties in terms of the coherence
proof.
Like the elaboration semantics of \fnamee, \citeauthor{pierce1991programming}
translates $\mathsf{F}_{\land}$ to System F extended with products, but he left
coherence as a conjecture.

More recently, \citet{castagna2014polymorphic} proposed a polymorphic calculus
with set-theoretic type connectives (intersections, unions, negations). But
their calculus does not include a merge operator. Compared to \fnamee, their
intersections are used between function types, allowing overloading of types, as shown below
(altering their syntax slightly):
\begin{lstlisting}[language=Haskell]
even :: (Int -> Bool) & ((a \ Int) -> (a \ Int))
even x = case x of
           | Int -> x `mod` 2 == 0
           | _   -> x
\end{lstlisting}
The above function operates differently according to the type of the argument:
it checks whether an argument is an integer; if it is so it returns whether the
integer is even or not, otherwise it returns the argument as it received. Note
that type difference (i.e., \lstinline{a \ Int}) is crucial to ensure no
ambiguity in the domain types of two functions. In \fnamee, we cannot express
this kind of intersections. However, \fnamee allows some other intersections
(e.g., $[[(nat -> bool) & (nat -> nat)]]$) that are not allowed in their system.
Nevertheless, both systems need to express negative information about type
variables: in their system type difference (e.g., \lstinline{a \ Int}) achieves
this, whereas in \fnamee we use disjointness constraints (e.g., \lstinline{a * Int}).
On a more theoretical note, \citet{castagna2014polymorphic} adopt the
\emph{semantic} approach for defining the subtyping relation, where one first
chooses a model, and an interpretation of types as subsets of the model, then
the subtyping relation can be defined as the inclusion of denoted sets. The
benefit of this approach, compared with the more used \emph{syntactic}
approach, is that the subtyping relation is by definition \emph{complete}. In
that regard, their subtyping relation thus completely subsumes BCD subtyping.


The combination of intersection types, a merge operator and parametric
polymorphism, while achieving coherence was first studied in the \fname
calculus~\citep{alpuimdisjoint}, which serves as a foundation for our \fnamee
calculus. Compared to \fname, the essential novelty is a BCD subtyping, and a
more powerful proof method for proving coherence. As far as we know, we are the
first to study the metatheory of the combination of BCD subtyping, parametric
polymorphism and the merge operator.



\section{Intersection Types and Multiple Inheritance}

\citet{compagnoni1996higher} proposed a lambda calculus
$\mathsf{F}_{\land}^{\omega}$, an extension of System $\mathsf{F}^{\omega}$ with
intersection types to model multiple inheritance. $\mathsf{F}_{\land}^{\omega}$
allows arbitrary finite intersections, where all the type members must have the
same kind. On the language side, modern object-oriented languages such as Scala, TypeScript,
Flow, Ceylon, and Grace have adopted some form of intersection types. Notably,
the DOT calculus~\citep{amin2012dependent,Rompf_2016}---a new type-theoretic
foundation for Scala---has a native support for intersection types. Generally
speaking, the most significant difference between our calculi and those
languages/calculi is that they do not have an explicit introduction form of
intersection types, like our merge operator. The lack of a native merge operator
leads to some awkward and type-unsafe solutions for defining a merge operator in
those languages. As noted by \citet{alpuimdisjoint}, one important use of
intersection types in TypeScript is the following function:
\begin{lstlisting}[language=JavaScript]
function extend<T, U>(first: T, second : U) : T & U {...}
\end{lstlisting}
which is analogous to our merge operator in that it takes two objects and
produces an object with the intersection of the types of the argument objects.
The implementation of \lstinline{extend} relies on low-level (and type-unsafe)
features of JavaScript. Similar encodings have also been proposed for Scala to
enable applications where the merge operator plays a fundamental
role~\citep{oliveira2013feature, rendel14attributes}. As we have shown in
\cref{sec:poly:motivation}, with disjointness constraints and a built-in merge
operator, a type-safe and conflict-free \lstinline{extend} function can be
naturally defined.



\section{Row Polymorphism and Extensible Records}
\label{sec:related:row}

Row polymorphism, first proposed by \citet{wand1987complete}, was intended as a
mechanism to enable type inference for a simple object-oriented language based
on recursive records. These ideas were later adopted into type systems for
extensible records~\citep{Harper:1991:RCB:99583.99603, leijen2005extensible,
  gaster1996polymorphic}. \citet{cardelli1989operations} define three primitive
operations on records: \emph{selection}, \emph{restriction} and
\emph{extension}. In our calculi, the merge operator can be regarded as a
generalization of record extension/concatenation, and selection is also
supported natively. In contrast to most record systems, restriction is not a
primitive operation in our calculi, but can be simulated via subtyping.
According to \citet{leijen2005extensible}, when it comes to extension, record
calculi can be divided into those that support \emph{free} extension, and
those that support \emph{strict} extension. The former allows duplicate labels
to coexist, whereas the latter does not. As pointed out by
\citet{alpuimdisjoint}, our calculi can be thought as a hybrid of these two
approaches: we allow duplicate labels as long as their types are disjoint. This
is more flexible than strict extension, but less expressive than
\citeauthor{leijen2005extensible}'s system where it also accepts duplicate
fields even when their types are overlapping. We refer to \citet{alpuimdisjoint}
for a detailed account of encodings of polymorphic extensible records using
disjoint polymorphism.


Row polymorphism alone cannot express the merge operator, as it only operates on
records (possibly with statically unknown fields). This essentially limits its
applications to extensible designs (such as defining Object Algebra combinators
in \cref{chap:case_study}). In this sense, we believe disjoint polymorphism is
more expressive than row polymorphism. It would be interesting to rigorously
study the relationship between disjoint polymorphism and row polymorphism,
whether the former subsumes the latter. We have some further discussions about
this point in \cref{sec:future:row}.



\section{Typed First-Class Classes/Mixins/Traits}

First-class classes have been used in Racket~\citep{DBLP:conf/aplas/FlattFF06},
along with mixin support, and have shown great practical value. For example,
DrRacket IDE~\citep{DBLP:journals/jfp/FindlerCFFKSF02} makes extensive use of
layered combinations of mixins to implement text editing features. The topic of
first-class classes with static typing has been explored by
\citet{DBLP:conf/oopsla/TakikawaSDTF12} in Typed Racket. They designed a gradual
type system that supports first-class classes. Of particular interest is their
use of row polymorphism to type mixins. For example, \lstinline{modal_mixin}
from \cref{sec:trait:overview} implemented in Typed Racket has type:
\begin{lstlisting}
(All (r / on-key toggle-mode)
     (Class ([on-key : (String -> Void)] | r)) ->
     (Class ([toggle-mode : (-> Void)] [on-key : (String -> Void)] | r)))
\end{lstlisting}
As with our use of disjoint polymorphism, row polymorphism can express
constraints on the presence or absence of members.
% Unlike disjoint polymorphism,
% row polymorphism prohibits forgetting class members. While this is reasonable in
% the setting of mixins, in some cases, a function taking one class as an argument
% can return another class that has fewer methods. For example, in \sedel we can write:
% \lstinputlisting[linerange=100-100]{./examples/overview2.sl}% APPLY:linerange=FOO
% where \lstinline{foo} drops \lstinline{bar} from its argument trait
% \lstinline{t}, which is impossible to express using row polymorphism.
As a consequence, Typed Racket ends up with two subtyping mechanisms: one for
first-class classes (via row polymorphism) and the other for objects (via normal
width subtyping). In contrast, \sedel uses only one mechanism---i.e., disjoint
polymorphism---to deal with both.


More recently, \citet{DBLP:conf/ecoop/LeeASP15} proposed a model for typed
first-class classes based on tagged objects. Like our development, the semantics
of their source language is defined by a translation into a target language. One
notable difference to \sedel is that they require the use of a variable rather
than an expression in the \lstinline{extends} clause, whereas we do not have
this restriction. In their source language, subclasses define subtypes, which
limits its applicability to extensible designs. Also their target calculus is
significantly more complex than ours due to the use of dependent function types
and dependent sum types. As they admitted, they omit inheritance in their
formalization.

Racket also supports a \emph{dynamically typed model} of first-class
traits~\citep{DBLP:conf/aplas/FlattFF06}. However, unlike Racket's first-class
classes and mixins, there is no type system supporting the use of first-class
traits. A key difficulty is \emph{statically} detecting conflicts. In the mixin
model this is not a problem because conflicts are implicitly resolved using the
order of composition. As far as we know, \sedel is the first design for typed
first-class traits.



\section{Mixin-Based Inheritance}

\citeauthor{bracha1990mixin}'s seminal paper~\citep{bracha1990mixin} extends
Modula-3 with mixins. Since then, many mixin-based models have been
proposed~\citep{flatt1998classes,bono1999core, ancona2003jam}. Mixin-based
inheritance requires that mixins are composed linearly, and as such, conflicts
are resolved implicitly. In comparison, the trait model in \sedel requires
conflicts to be resolved explicitly. We want to emphasize that conflict
detection is essential in expressing composition operators for Object Algebras,
without running into ambiguities. \citeauthor{bracha1992programming}'s Jigsaw
framework~\citep{bracha1992programming} formalized mixin composition, along with
a rich trait algebra including merge, restrict, select, project, overriding and
rename operators. \citet{LAGORIO201286} proposed \textsc{FJig} that reformulates
Jigsaw constructs in a Java-like setting. \citet{DBLP:conf/oopsla/AllenBC03}
described how to add first-class generic types---including mixins---to object-oriented
languages with nominal typing. \citet{Corradi_2012} described an extension of
\textsc{FJig} that integrates modular composition and nesting of Java-like
classes. It features a set of composition operators that allow to manipulate
nested classes at any depth level. In all of these systems, classes and mixins,
though they enjoy static typing, are still second-class constructs, and thus
their systems cannot express dynamic inheritance.


\section{Trait-Based Inheritance}

Traits were originally proposed by \citet{scharli2003traits}, and later formalized by ~\cite{Ducasse_2006} as a mechanism
for fine-grained code reuse to overcome many limitations of class-based
inheritance. The original proposal of traits were implemented in the
dynamically typed class-based language \textsc{Squeak/Smalltalk}. Since then
various formalizations of traits in a Java-like (statically typed) setting have
been proposed~\citep{fisher2004typed,scharli2003traitsformal,chai_trait,
  JOT:issue_2006_05/article4}. In most of the above proposals, trait composition
complements class-based inheritance. \sedel, in the spirit of \emph{pure trait-based programming languages}~\citep{BETTINI2013521, BETTINI2017419},
embraces traits as the sole mechanism for code reuse. The deviation from
traditional class-based inheritance is not only because of its simplicity, but
also because we need a very \emph{dynamic} form of inheritance. In comparison to
the traditional trait mode, traits in \sedel have the following differences:
\begin{enumerate}
\item traditional traits cannot be instantiated but only composed with a class,
  whereas traits in \sedel can be instantiated directly;
\item traditional traits cannot take constructor parameters whereas ours can;
\item the trait system in \sedel lacks a proper notion of inheritance
  relationship, e.g., in the traditional trait model, if the \emph{same}
  method is obtained more than once via different paths, there is no conflict.
  This is not the case in \sedel;
\item and finally traits in \sedel are first-class and support dynamic
  inheritance.
\end{enumerate}




\section{Family Polymorphism}

There has been much work on family polymorphism since \citeauthor{Ernst_2001}'s
original proposal~\citep{Ernst_2001}. Family polymorphism provides an elegant
solution to the expression problem. Although a simple Scala solution does exist
without requiring family polymorphism (e.g., see \citet{wang2016expression}), Scala does not support nested composition:
programmers need to manually compose all the classes from multiple extensions.
Generally speaking, systems that support family polymorphism can be divided into
two categories: those that support \emph{object families} and those that
support \emph{class families}. The original object family approach of
\textsc{Beta }(e.g., virtual classes~\citep{Madsen_1989}) treats nested classes as
attributes of objects of the family classes, whereas in class families,
classes are nested in other classes. The former choice is considered more
expressive~\citep{ErnstVirtual}, but requires a complex type system usually
involving dependent types. The object and class family approaches have even been
combined by the work on Tribe~\citep{pubsdoc:tribe-virtual-calculus}.

\paragraph{Object families.}

Virtual classes~\citep{Madsen_1989} as introduced in
\textsc{Beta}~\citep{LehrmannMadsen:1993:OPB:221048} are based on object
families. However, the virtual class mechanism in \textsc{Beta} is unsound.
Path-dependent types are used to ensure type safety for virtual types and
virtual classes in the calculus \textsc{vc}~\citep{ErnstVirtual}. One distinctive
difference from our calculi is that \textsc{vc} follows the mixin-style by
allowing the rightmost class to take precedence, whereas in \namee conflicts are
detected statically and resolved explicitly.

\paragraph{Class families.}

Concord~\citep{jolly2004simple}, Jx~\citep{Nystrom_2004} and
J\&~\citep{Nystrom:2006} follow the class family approach, where nested classes
and types are attributes of the family classes directly.
% Unlike virtual classes, subclass and subtype relationships are preserved by inheritance: the
% overriding class is also a subtype of the class it overrides.
%Nested inheritance
%does not support generic types. As we discussed in \cref{sec:diss}, \namee can be
%easily extended to incorporate parametric polymorphism.
Jx supports \emph{nested inheritance}, a class family mechanism that allows
nesting of arbitrary depth. J\& is a language that supports \emph{nested
  intersection}, building on top of Jx. Similar to our calculi, intersection
types play an important role in J\&, which are used to compose packages/classes.
However, J\& does not have a merge-like operator. When conflicts arise, prefix
types can be exploited to resolve the ambiguity. J\&$_s$~\citep{Qi:2009} is an
extension of the Java language that adds class sharing to J\&.
\citet{SAITO_2007} identified a minimal, lightweight set of language features to
enable family polymorphism,

Compared with those systems, which usually focus on getting a relatively complex
Java-like language with family polymorphism, our work on \namee focuses on a
minimal calculus that supports nested composition. We have shown that a calculus
with the merge operator and a variant of BCD subtyping captures the essence of nested
composition. Moreover, \namee enables new insights on the subtyping relations of
families. Our goal in this thesis is not to support full family polymorphism
which, besides nested composition, also requires dealing with other features
such as self types~\citep{bruce95thistype,saito09matching} and mutable state.
But we expect to investigate those features in the future.




\section{Languages with More Advanced Forms of Inheritance}

\textsc{Self}~\citep{ungar1988self} is a dynamically typed, prototype-based
language with a simple and uniform object model. \textsc{Self}'s inheritance
model is typical of what we call \emph{mutable inheritance}, because an
object's parent slot may be assigned new values at run time. Mutable inheritance
is rather unstructured, and oftentimes access to any clashing methods will
generate a ``messageAmbiguous'' error at run time. Although \sedel's dynamic
inheritance is not as powerful as mutable inheritance, its static type system
can guarantee that no such errors occur at run time.
Eiffel~\citep{meyer1987eiffel} supports a sophisticated class-based multiple
inheritance with deep renaming, exclusion and repeated inheritance. Of
particular interest is that in Eiffel, name collisions are considered
programming errors, and ambiguities must be resolved explicitly by the
programmer (by means of renaming). In this regard, \sedel is quite like Eiffel.
However, the type system in \sedel is more lenient in that two identically named
methods with different signatures can coexist.
Grace~\citep{DBLP:journals/jot/NobleBBHJ17, DBLP:conf/ecoop/0002HNB16} is an
object-based language designed for education, where objects are created by
\emph{object constructors}.
% In that regard, Grace is similar to \namee in that
% both are not class-based.
Since Grace has mutable fields, it has to consider
many concerns when it comes to inheritance, resulting in a rather complex
inheritance mechanism with various restrictions.
% For example, Grace imposes the
% constraint that the object being inherited must be \emph{fresh}. The effect of
% the freshness constraint is that the expression in the inherit clause must
% generate a new object. As a consequence, the expression after
% \lstinline{inherit} could not be a variable, which seems to preclude
% dynamic inheritance.
Since \sedel is pure, a relatively simple
% delegation-based
encoding of traits with late binding of \lstinline{self}
suffices for our applications. Grace's support for multiple inheritance is
based on what they call \emph{instantiable traits}.
% Still the freshness constraint applies.
We believe that there is plenty to be learned from Grace's design of traits if
we want to extend our trait model with features such as mutable state.
\textsc{MetaFJig}~\citep{SERVETTO2014219} (an extension of \textsc{FJig})
supports \emph{dynamic trait replacement}~\citep{chai_trait, BETTINI2013907,
  Ducasse_2006}, a feature for changing the behavior of an object at run time by
replacing one trait for another. More recently, a Java-like language called
Familia~\citep{Zhang_2017} were proposed to combine subtyping polymorphism,
parametric polymorphism and family polymorphism.



\section{Module Systems}

In parallel to OOP, the ML module system originally proposed by
\citet{MacQueen_1984} also offers powerful support for flexible program
construction. There is a large body of work on ML modules. Supporting
\emph{data abstraction} is the primary focus of the module mechanism in ML. It
ensures \emph{implementer-side} data abstraction by allowing the implementer
of a module to ``hide'' specific implementation behind an abstract interface. It
also supports a form of \emph{client-side} data abstraction where a client can
develop and compile a module independently from the modules on which it depends,
via the ``functor'' mechanism. One major limitation of the traditional ML module
systems is the lack of support for mutually recursive modules. There are several
proposals of extending ML with recursive
modules~\citep{Crary_1999,Rossberg_2013, Russo_2001}. Mixin modules in the
Jigsaw framework~\citep{Bracha92modularitymeets} provides a suite of operators
for adapting and combining modules. The MixML module
system~\citep{Rossberg_2013} incorporates mixin module composition, while
retaining the full expressive powerful of ML modules. There is also work on
elaborating the semantics of module systems into a smaller, well-established
internal language. \citet{ROSSBERG_2014} showed that plain System F is
sufficient as an internal language for conventional ML modules.
Furthermore, \citet{Rossberg_2015} proposed a redesign of ML in which modules are truly
first-class values, thus unifying the core and module layers into one language.

Module systems usually put more emphasis on supporting data abstraction. Support
for data abstraction adds considerable complexity, which is not needed in
\sedel. \sedel is focused on OOP and supports, among others, method overriding,
self references and dynamic dispatching, which (generally speaking) are all
missing features in module systems.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../Thesis"
%%% org-ref-default-bibliography: ../Thesis.bib
%%% End:
